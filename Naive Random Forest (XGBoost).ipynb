{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dask.array as da\n",
    "import h5py as h5\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import fbeta_score, accuracy_score\n",
    "# import dask_xgboost as dxgb\n",
    "from dask_ml.xgboost import XGBClassifier\n",
    "from dask_ml.preprocessing import DummyEncoder\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Start client\n",
    "# client = Client('10.70.1.160:8786')\n",
    "# client = Client(processes=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "DATA_PATH = r'../data/eightieth.h5' # Path to the learning dataset\n",
    "CHUNK_SIZE = 5000 # Number of images to process in one batch (must fit comfortably in memory)\n",
    "TRAIN_SIZE = 7/8 # Size of the train dataset (compared to the total dataset)\n",
    "\n",
    "\n",
    "# data_h5 = h5.File('data/eightieth.h5', mode='r')\n",
    "# landcover = da.from_array(data_h5['TOP_LANDCOVER'], chunks=(1000,1))\n",
    "# images = da.from_array(data_h5['S2'], chunks=(1000,16,16,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup generator yielding X, y chunks (default size 1000)\n",
    "\n",
    "classes = da.unique(da.from_array(h5.File(DATA_PATH)['TOP_LANDCOVER'], chunks=(CHUNK_SIZE,1))).compute()\n",
    "\n",
    "def generator(h5_path, chunk_size, train_size):\n",
    "    f = h5.File(h5_path, 'r')\n",
    "    X = f['S2']\n",
    "    y = f['TOP_LANDCOVER']\n",
    "    \n",
    "    # Compute number of chunks needed\n",
    "    no_chunks = len(X) // chunk_size\n",
    "    if len(X) % chunk_size > 0:\n",
    "        # len(X) is not an exact multiple of chunk_size\n",
    "        no_chunks += 1\n",
    "    \n",
    "    print(len(X), '-', no_chunks)\n",
    "    for c in range(no_chunks):\n",
    "        X_to_yield = X[c*chunk_size:(c+1)*chunk_size,:,:,:]\n",
    "        y_to_yield = y[c*chunk_size:(c+1)*chunk_size,:]\n",
    "        \n",
    "        # Wrangle data\n",
    "        X_to_yield = np.array(X_to_yield).reshape((-1, 16*16*4)) # Flatten each element in the array\n",
    "#         y_to_yield = pd.get_dummies(np.array(y_to_yield).astype(int).reshape((-1,)))\n",
    "        y_to_yield = np.array(y_to_yield).astype(int).reshape((-1,))\n",
    "        y_to_yield = label_binarize(y_to_yield, classes=classes)\n",
    "#         y_to_yield = pd.DataFrame(y_to_yield, index=None, columns=['landcover']).astype(int)\n",
    "#         y_to_yield = pd.get_dummies(y_to_yield, columns=['landcover'])\n",
    "        \n",
    "        \n",
    "        yield train_test_split(X_to_yield, y_to_yield, train_size=train_size, test_size=1-train_size)\n",
    "    \n",
    "# Test\n",
    "# for X, y in generator('data/eightieth.h5', 1000):\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "234000 - 47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "47it [01:14,  1.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean score: 0.45891914893617025\n"
     ]
    }
   ],
   "source": [
    "forests_list = [] # List of forests to aggregate\n",
    "\n",
    "for X_train, X_test, y_train, y_test in tqdm(generator(DATA_PATH, CHUNK_SIZE, TRAIN_SIZE)):\n",
    "    cf = RandomForestClassifier(n_estimators=10, n_jobs=-1) # parameters to define\n",
    "    cf.fit(X=X_train, y=y_train)\n",
    "    \n",
    "#     Reshape y_test to make it understandable (1D series)\n",
    "#     y_test = y_test.idxmax(axis=1)\n",
    "    score = cf.score(X_test, y_test)\n",
    "    \n",
    "    forests_list.append((cf, score))\n",
    "    \n",
    "forests_list = np.array(forests_list)\n",
    "print('Mean score:', forests_list[:,1].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Custom VotingClassifier\n",
    "# Can ensemble pre-fitted models (as opposed to sklearn's implementation which needs retraining)\n",
    "\n",
    "class VotingClassifier:\n",
    "    # Two voting modes :\n",
    "        # - Hard: the majority wins, if there's a tie, a contender is selected randomly\n",
    "        # - Soft: use the provided weights and tally the weighted votes to determine winners. Weights must have shape (len(estimators))\n",
    "    \n",
    "    def __init__(self, estimators, classes=None, weights=None):\n",
    "        # estimators: iterable of estimators implementing the predict() API\n",
    "        # weights: iterable of (numeric) weights\n",
    "        # classes: list of matching classes\n",
    "        \n",
    "        self.estimators = estimators\n",
    "        self.weights = np.array(weights)\n",
    "        self.n_classes = len(self.estimators[0].n_classes_)\n",
    "        \n",
    "    \n",
    "    def _special_argmax(self, pred):\n",
    "        # Compute the usual argmax along the lines\n",
    "        # For each null predictions ([0]*no_features), replace the argmax with -1 to filter it easily\n",
    "        # Having an argmax of 0 then actually means that the predicted label is the #0\n",
    "\n",
    "        # pred shape=(no_classifiers, no_features)\n",
    "        out = pred.argmax(axis=1)\n",
    "        out[np.all(pred==-1.0, axis=1)] = self.n_classes + 1\n",
    "\n",
    "        return out\n",
    "\n",
    "    def predict(self, X):\n",
    "        # X is of shape (no_samples, no_features)\n",
    "        # Return array of shape (no_samples,)\n",
    "\n",
    "        predictions = np.empty((len(self.estimators), X.shape[0], self.n_classes))\n",
    "        \n",
    "        for i, clf in enumerate(self.estimators):\n",
    "            predictions[i, :, :] = clf.predict(X)\n",
    "            \n",
    "        # predictions is of shape (no_classifiers, no_samples, no_features)\n",
    "        \n",
    "        out = np.zeros((X.shape[0],)) # out matrix\n",
    "        \n",
    "        for k in range(predictions.shape[1]):\n",
    "            arr = predictions[:, k, :]\n",
    "            \n",
    "            # Replace all the 0 to -1 (in order to filter out the null prediction later on)\n",
    "            np.place(arr, arr==0.0, -1)\n",
    "            \n",
    "            arr_label = self._special_argmax(arr)\n",
    "            \n",
    "            idx_label = ~(arr_label == self.n_classes + 1)\n",
    "            \n",
    "            out[k] = np.argmax(np.bincount(arr_label[idx_label], \n",
    "                                           weights=weights[idx_label]))\n",
    "            \n",
    "        return out    \n",
    "    \n",
    "    def score(self, X, y):\n",
    "        # Convert y to its category label\n",
    "        y = y.argmax(axis=1)\n",
    "        y_pred = self.predict(X)\n",
    "        \n",
    "        accuracy = accuracy_score(y, y_pred)\n",
    "        fbeta = fbeta_score(y, y_pred, 2, average='macro')\n",
    "        \n",
    "        print('Accuracy :', accuracy)\n",
    "        print('F2 score :', fbeta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Miniconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Program Files\\Miniconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.356\n",
      "F2 score : 0.18251701906413356\n"
     ]
    }
   ],
   "source": [
    "vc = VotingClassifier(forests_list[:,0], weights=[1]*len(forests_list[:,0]))\n",
    "# predictions = vc.predict(X_test)\n",
    "vc.score(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
